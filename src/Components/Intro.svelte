
<section>
  <p class="body-text">
    Imagine you're lost in a foreign city, and you need to find a restaurant. 
    What do you do? You probably look for the nearest people and ask for their 
    recommendations. That's essentially how the k-nearest neighbors (KNN) algorithm 
    works - it classifies unknown data points based on the majority class of 
    their nearest neighbors.
  </p>
  <br>
  <p class="body-text">

    <a href="https://www.ibm.com/topics/knn">The k-nearest neighbors</a>
    algorithm is one of the simplest classification and regression classifiers. 
    It is a non-parametric algorithm, which means that it does not assume any 
    specific data distribution. It operates based on the idea that similar data 
    points are close to each other in a multi-dimensional space. KNN classifies 
    a new data point based on the majority class of its k nearest neighbors. 
    k controls the complexity of the decision boundary, thus influencing the 
    overall performance of the model.

  </p>
</section>

<style>
</style>
